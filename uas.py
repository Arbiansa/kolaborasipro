# -*- coding: utf-8 -*-
"""uas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XPv1aarHW2sKS0QPbFFtaYtwc9_F92g9

# **UAS PSD**

NPM     : 200411100195

Nama    : Muhammad Arbiansyafa Siswanto (Ketua)

NPM     : 200411100118

Nama    : Bachtiar Dwi Yusril R. (Anggota)

GitHub  : https://github.com/Arbiansa/kolaborasipro

# Get and prepare Data
"""

import numpy as np
import pandas as pd

import pandas as pd

# URL file CSV di repositori GitHub
csv_url = 'https://raw.githubusercontent.com/Arbiansa/kolaborasipro/main/PLN%3DX.csv'

# Membaca file CSV dari URL menggunakan pandas
df_data = pd.read_csv(csv_url)

# Menampilkan data dari file CSV
df_data.head(7)

df_data.shape

df_close= df_data['Close']

# transform univariate time series to supervised learning problem
from numpy import array
# split a univariate sequence into samples
def split_sequence(sequence, n_steps):
    X, y = list(), list()
    for i in range(len(sequence)):
    # find the end of this pattern
        end_ix = i + n_steps
    # check if we are beyond the sequence
        if end_ix > len(sequence)-1:
            break
    # gather input and output parts of the pattern
        # print(i, end_ix)
        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
        X.append(seq_x)
        y.append(seq_y)
    return array(X), array(y)

"""# Tuning data"""

n_steps = 5
X, y = split_sequence(df_close, n_steps)

print(X.shape, y.shape)

n_steps = 5
X, y = split_sequence(df_close, n_steps)  # column names to X and y data frames
df_X = pd.DataFrame(X, columns=['t-' + str(i) for i in range(n_steps-1, -1, -1)])
df_y = pd.DataFrame(y, columns=['t+1 (prediction)'])

# concat df_X and df_y
df = pd.concat([df_X, df_y], axis=1)

print(X.shape, y.shape)

df.head(5)

"""# Plotting"""

import matplotlib.pyplot as plt

# Membuat subplot dengan lebar sumbu x yang lebih besar
fig, ax = plt.subplots(figsize=(12, 6))

# Plotting
ax.plot(df.index, df['t+1 (prediction)'], label='t+1 (prediction)', linestyle='-', color='blue')
for i in range(n_steps):
    ax.plot(df.index, df['t-' + str(i)], label='t-' + str(i), linestyle='--', marker='o')

ax.set_xlabel('Index')
ax.set_ylabel('Value')
ax.legend()
plt.show()

"""# Preprocessing"""

from sklearn.preprocessing import MinMaxScaler
scaler= MinMaxScaler()
X_norm= scaler.fit_transform(df_X)
# y_norm= scaler.fit_transform(df_y)

X_norm

"""## Split data"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2, random_state=0)

"""## Model KNN"""

# import knn
from sklearn.neighbors import KNeighborsRegressor
model_knn = KNeighborsRegressor(n_neighbors=7)

"""## Fit dan Prediksi KNN"""

model_knn.fit(X_train, y_train)
y_pred=model_knn.predict(X_test)

"""## MSE

$$ \text{MSE} = \frac{1}{n} \sum_{i=0}^n (y_i - \hat{y}_i)^2$$
"""

from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, y_pred)

"""## Hasil Mape"""

from sklearn.metrics import mean_absolute_percentage_error

# Assuming you have the true target values (y_test) and the predicted values (y_pred)
mape = mean_absolute_percentage_error(y_test, y_pred)
print("Mean Absolute Percentage Error:", mape)

"""## Result"""

y_test.shape

y_pred.shape

df_y_test = pd.DataFrame(y_test,columns=['y_test'])
df_y_pred = pd.DataFrame(y_pred,columns=['y_pred'])

df_hasil = pd.concat([df_y_test, df_y_pred], axis=1)

df_hasil

# df_hasil.to_excel('df_hasil n_step={}.xlsx'.format(n_steps), index=False)

y.min()

y.max()

"""## Model Naive Bayes"""

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error


# Create a Naive Bayes
naive_bayes = GaussianNB()

"""## Fit & Predict Naive Bayes"""

import numpy as np

# Define the bin edges or thresholds
bin_edges = [4.0, 4.5, 5.0]  # Adjust the values based on your requirements

# Perform binning on the labels
y_train_categorical = np.digitize(y_train, bin_edges)

# Create a Naive Bayes classifier
naive_bayes = GaussianNB()

# Training the model
naive_bayes.fit(X_train, y_train_categorical)

# Making predictions on the test set
y_pred = naive_bayes.predict(X_test)

"""## Plotting the predicted values and actual values"""

# Plotting the predicted values and actual values
plt.figure(figsize=(8, 6))
plt.plot(np.arange(len(y_test)), y_test, label='Actual')
plt.plot(np.arange(len(y_test)), y_pred, label='Predicted')
plt.xlabel('Index')
plt.ylabel('Value')
plt.title('Actual vs Predicted')
plt.legend()
plt.show()

"""## MSE"""

# Calculating MSE
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error (MSE):", mse)

"""## Mape"""

# Calculating MAPE
mape = mean_absolute_percentage_error(y_test, y_pred)
print("Mean Absolute Percentage Error (MAPE):", mape)

"""## Model Random Forest Model"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error

# Create a Random Forest Regressor
random_forest = RandomForestRegressor()

# Train the model
random_forest.fit(X_train, y_train)

# Make predictions
predictions = random_forest.predict(X_test)

# Calculate evaluation metrics
mse = mean_squared_error(y_test, predictions)

print("Mean Squared Error (MSE):", mse)

mape = mean_absolute_percentage_error(y_test, predictions)
print("Mean Absolute Percentage Error (MAPE):", mape)